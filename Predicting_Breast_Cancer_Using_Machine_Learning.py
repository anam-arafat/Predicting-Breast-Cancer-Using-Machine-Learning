# -*- coding: utf-8 -*-
"""Learn To Predict Breast Cancer Using Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VqVB13DQt_u7FaJ-C-MU0VZvebqwOk1-

Dataset: [https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data](https://)
"""

#Downloading Python modules
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install seaborn

#Importing Python modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Loading dataset into the system (Method 1): Upload dataset to colab and read
df = pd.read_csv('/content/data.csv')
df.head()

#Loading dataset into the system (Method 2): Upload dataset from the system
from google.colab import files
uploaded = files.upload()

#Reading the file
df_1 = pd.read_csv('/content/data (1).csv')
df_1.head()

#Loading dataset into the system (Method 3): Downloading dataset from web
#Install Kaggle library
!pip install kaggle

#Make Kaggle directory
!mkdir ~/.kaggle

#Copy the "kaggke.json" into this new directory
!cp kaggle.json ~/.kaggle/

#Allocating the required permission for tis file
!chmod 600 ~/.kaggle/kaggle.json

#Downloading the dataset
!kaggle datasets download uciml/breast-cancer-wisconsin-data

#Extracting the data.csv file
!unzip breast-cancer-wisconsin-data.zip

#Reading the file
df = pd.read_csv('/content/data.csv')
df.head()

"""**Exploratory Data Analysis (EDA)**"""

#Total no. of rows and columns
df.shape

#Checking columns and their corresponding data types
df.info()

#Check for null values
df.isnull().sum()

#Remove the column with all missing values
df = df.dropna(axis=1)

#Checking all data after deleting column
df.shape
df.info()
df.isnull().sum()

#Checking total number of each data in 'diagnosis' column
df['diagnosis'].value_counts()

#Graphical representation of data count
sns.countplot(df['diagnosis'], label='count')

#Library for transforming categorical data into numerical data
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()

#Transforming categorical data into numerical data
df.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values)

#New 'diagnosis' column where B = 0, M = 1
df.iloc[:,1].values

#Plotting out relationships between 'diagnosis' and other columns
sns.pairplot(df.iloc[:,1:6], hue='diagnosis')

#Correlation between columns
df.iloc[:,1:12].corr()

#Heatmap to understand correlation
plt.figure(figsize=(8,8))
sns.heatmap(df.iloc[:,1:12].corr(), annot=True, fmt='.0%')

#Splitting our dataset into independent and dependent datasets (Feature Scaling)
X = df.iloc[:,2:31].values #Independent
Y = df.iloc[:,1].values #Dependent

#80:20 ratio
from sklearn.model_selection import train_test_split
threshold = 0.5  # Example threshold, adjust as needed
Y = np.where(Y > threshold, 1, 0)  # Convert continuous target to binary classes
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)

#Standardizing numerical values
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Building Machine Learning models
def models(X_train, Y_train):

  #Logistic Regression
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state = 0)
  log.fit(X_train, Y_train)

  #Decision Tree
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(X_train, Y_train)

  #Random Forest
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, Y_train)

  #Printing accuracy of each model on the training dataset
  print('Logistic Regression Classifier Training Accuracy: ', log.score(X_train, Y_train))
  print('Decision Tree Classifier Training Accuracy: ', tree.score(X_train, Y_train))
  print('Random Forest Classifier Training Accuracy: ', forest.score(X_train, Y_train))

  return log, tree, forest

#Printing accuracy of models
model = models(X_train, Y_train)

#Constructing confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, model[0].predict(X_test))
tp = cm[0][0] #True Positive
fn = cm[0][1] #False Negative
fp = cm[1][0] #False Positive
tn = cm[1][1] #True Negative
print(cm)
print('Accuracy = ', (tp+tn)/(tp+tn+fp+fn))

#Classification and Accuracy Report of all models
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model ', i)
  print(classification_report(Y_test, model[i].predict(X_test)))
  print(accuracy_score(Y_test, model[i].predict(X_test)))
  print()

#Model Prediction vs Actual Prediction
pred = model[2].predict(X_test)
print('Our model prediction: ')
print(pred)
print()
print('Actual prediction: ')
print(Y_test)